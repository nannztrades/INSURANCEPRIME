
# src/api/ingestion_api.py
# pyright: reportCallIssue=false
from __future__ import annotations
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from src.ingestion.parser_db_integration import ParserDBIntegration
from src.ingestion.run_logger import RunLogger
from src.ingestion.commission import compute_expected_for_upload_dynamic, insert_expected_rows
from src.ingestion.db import get_conn
import src.parser.parser_db_ready_fixed_Version4 as parser_v4

router = APIRouter(prefix="/api/ingestion", tags=["Ingestion"])

MAX_BYTES = 10 * 1024 * 1024  # 10 MB

def _as_int(value: Any) -> Optional[int]:
    if isinstance(value, int):
        return value
    if isinstance(value, str):
        try:
            return int(value)
        except ValueError:
            return None
    try:
        return int(value)
    except Exception:
        return None

def _parse_with_v4(func_name: str, path: str) -> Any:
    fn = getattr(parser_v4, func_name, None)
    if fn is None or not callable(fn):
        raise HTTPException(status_code=500, detail=f"Parser function '{func_name}' not available.")
    return fn(path)

def _ensure_pdf_upload(file: UploadFile, content_bytes: bytes) -> None:
    # Content-Type must be application/pdf
    ct = (file.content_type or "").lower().strip()
    if ct != "application/pdf":
        raise HTTPException(status_code=422, detail="Only application/pdf uploads are accepted.")
    # Size â‰¤ 10MB
    if len(content_bytes) > MAX_BYTES:
        raise HTTPException(status_code=422, detail="File too large (max 10 MB).")

def _auto_deactivate_prior_active(agent_code: str, month_year: str, doc_type: str, current_upload_id: int) -> None:
    conn = get_conn()
    try:
        with conn.cursor() as cur:
            # Set all other actives for same tuple to inactive
            cur.execute(
                """
                UPDATE `uploads`
                SET `is_active`=0
                WHERE `agent_code`=%s AND `month_year`=%s AND `doc_type`=%s
                  AND `UploadID`<>%s AND `is_active`=1
                """,
                (agent_code, month_year, doc_type, current_upload_id),
            )
            # Ensure the current one is active (defensive)
            cur.execute(
                "UPDATE `uploads` SET `is_active`=1 WHERE `UploadID`=%s",
                (current_upload_id,),
            )
        conn.commit()
    finally:
        conn.close()

def _refresh_active_policies_for(agent_code: str, month_year: str) -> None:
    """
    Refresh snapshot for active_policies scoped to the agent+month window.
    Strategy:
      - Upsert rows for policies seen in the statement of the month:
        last_seen_date = MAX(pay_date) in that month
        last_premium   = premium at last_seen_date (approx: MAX(premium))
        last_com_rate  = MAX(com_rate)
        status='ACTIVE'
      - Reset consecutive_missing_months to 0 for those seen this month.
    """
    conn = get_conn()
    try:
        with conn.cursor() as cur:
            # Upsert visible policies for this month
            cur.execute(
                """
                INSERT INTO `active_policies`
                  (`agent_code`,`policy_no`,`policy_type`,`holder_name`,
                   `last_seen_date`,`last_seen_month_year`,`last_premium`,`last_com_rate`,
                   `status`,`consecutive_missing_months`)
                SELECT s.`agent_code`, s.`policy_no`, s.`policy_type`, s.`holder`,
                       MAX(s.`pay_date`) AS last_seen_date,
                       s.`MONTH_YEAR`    AS last_seen_month_year,
                       MAX(s.`premium`)  AS last_premium,
                       MAX(s.`com_rate`) AS last_com_rate,
                       'ACTIVE' AS status,
                       0 AS consecutive_missing_months
                FROM `statement` s
                WHERE s.`agent_code`=%s AND s.`MONTH_YEAR`=%s
                GROUP BY s.`agent_code`, s.`policy_no`, s.`policy_type`, s.`holder`, s.`MONTH_YEAR`
                ON DUPLICATE KEY UPDATE
                   `policy_type`=VALUES(`policy_type`),
                   `holder_name`=VALUES(`holder_name`),
                   `last_seen_date`=VALUES(`last_seen_date`),
                   `last_seen_month_year`=VALUES(`last_seen_month_year`),
                   `last_premium`=VALUES(`last_premium`),
                   `last_com_rate`=VALUES(`last_com_rate`),
                   `status`='ACTIVE',
                   `consecutive_missing_months`=0
                """,
                (agent_code, month_year),
            )
        conn.commit()
    finally:
        conn.close()

@router.get("/health")
def ingestion_health() -> Dict[str, Any]:
    return {"status": "ok", "module": "ingestion_api"}

@router.post("/one")
async def ingest_one(
    doc_type: str = Form(...),  # 'statement' | 'schedule' | 'terminated'
    file: UploadFile = File(...),
    agent_code: Optional[str] = Form(None),
    agent_name: Optional[str] = Form(None),
    month_year_hint: Optional[str] = Form(None),
    dry_run: bool = Form(False),
) -> Dict[str, Any]:
    project_root = Path(__file__).resolve().parents[2]
    logger = RunLogger(project_root)

    # Read file and enforce PDF rules (MIME + 10MB)
    content = await file.read()
    _ensure_pdf_upload(file, content)

    tmp = project_root / "tmp_ingestion_upload"
    tmp.mkdir(parents=True, exist_ok=True)
    filename = file.filename or "upload.pdf"
    target = tmp / filename
    with target.open("wb") as f:
        f.write(content)

    # Parse according to type
    doc = doc_type.lower().strip()
    if doc == "statement":
        df = _parse_with_v4("extract_statement_data", str(target))
    elif doc == "schedule":
        df = _parse_with_v4("extract_schedule_data", str(target))
    elif doc == "terminated":
        df = _parse_with_v4("extract_terminated_data", str(target))
    else:
        raise HTTPException(status_code=422, detail="Invalid doc_type")

    rows_raw: List[Dict[str, Any]] = [] if df is None else df.to_dict(orient="records")  # type: ignore[attr-defined]
    rows: List[Dict[str, Any]] = [{str(k): v for k, v in r.items()} for r in rows_raw]

    integ = ParserDBIntegration()
    try:
        summary = integ.process(
            doc_type_key=doc,
            agent_code=str(agent_code or ""),
            agent_name=agent_name or None,
            df_rows=rows,
            file_path=target,
            month_year_hint=month_year_hint or None,
        )
    except HTTPException:
        raise
    except Exception as e:
        # Log API-level failure
        logger.log_csv({
            "type": doc.upper(),
            "file": filename,
            "rows_parsed": "",
            "agent_code": agent_code or "",
            "agent_name": agent_name or "",
            "upload_id": "",
            "rows_inserted": "",
            "moved_to": "",
            "status": "api_error",
            "error": str(e),
        })
        logger.log_json({"status": "api_error", "error": str(e), "doc_type": doc.upper(), "agent_code": agent_code or "", "agent_name": agent_name or ""})
        raise HTTPException(status_code=500, detail=str(e))

    # Observability logs
    summary.setdefault("status", "success")
    logger.log_json(summary)
    logger.log_csv({
        "type": summary.get("doc_type", doc.upper()),
        "file": filename,
        "rows_parsed": len(rows),
        "agent_code": summary.get("agent_code", "") or (agent_code or ""),
        "agent_name": summary.get("agent_name", "") or (agent_name or ""),
        "upload_id": summary.get("upload_id", ""),
        "rows_inserted": summary.get("rows_inserted", 0),
        "moved_to": summary.get("moved_to", ""),
        "status": summary.get("status", "success"),
        "error": summary.get("error", ""),
    })

    # STATEMENT post-processing (only when real ingest succeeded)
    if (not dry_run and summary.get("doc_type") == "STATEMENT"
        and summary.get("upload_id") is not None
        and summary.get("status") == "success"):
        upid = _as_int(summary.get("upload_id"))
        agent = str(summary.get("agent_code") or agent_code or "")
        mmyy = str(summary.get("month_year") or month_year_hint or "")
        if upid is not None and agent and mmyy:
            # Compute dynamic expected commissions
            rows_exp = compute_expected_for_upload_dynamic(upload_id=upid)
            inserted = insert_expected_rows(rows_exp)
            summary["expected_rows_inserted"] = inserted

            # Auto-deactivate prior active uploads for (agent, month, doc_type)
            _auto_deactivate_prior_active(agent, mmyy, "STATEMENT", upid)

            # Refresh active_policies snapshot for that agent+month
            _refresh_active_policies_for(agent, mmyy)

    return summary

@router.post("/bulk")
async def ingest_bulk_dir(
    dir_path: str = Form(...),  # e.g., "data/incoming"
    override_agent_code: Optional[str] = Form(None),
    override_agent_name: Optional[str] = Form(None),
    dry_run: bool = Form(False),
) -> Dict[str, Any]:
    project_root = Path(__file__).resolve().parents[2]
    logger = RunLogger(project_root)

    base = Path(dir_path)
    if not base.exists() or not base.is_dir():
        raise HTTPException(status_code=404, detail=f"Directory not found: {base}")

    integ = ParserDBIntegration()
    results: List[Dict[str, Any]] = []

    for p in sorted(base.iterdir()):
        if not p.is_file(): 
            continue
        name = p.name.lower()
        if "statement" in name:
            doc = "statement"
            df = _parse_with_v4("extract_statement_data", str(p))
        elif "schedule" in name:
            doc = "schedule"
            df = _parse_with_v4("extract_schedule_data", str(p))
        elif "terminat" in name:
            doc = "terminated"
            df = _parse_with_v4("extract_terminated_data", str(p))
        else:
            continue

        try:
            rows_raw: List[Dict[str, Any]] = [] if df is None else df.to_dict(orient="records")  # type: ignore[attr-defined]
            rows: List[Dict[str, Any]] = [{str(k): v for k, v in r.items()} for r in rows_raw]

            if dry_run:
                summary = {
                    "status": "DRY_RUN",
                    "doc_type": doc.upper(),
                    "agent_code": override_agent_code or "",
                    "agent_name": override_agent_name or "",
                    "month_year": None,
                    "upload_id": None,
                    "rows_inserted": 0,
                    "moved_to": None,
                }
                results.append(summary)
                logger.log_csv({
                    "type": doc.upper(),
                    "file": p.name,
                    "rows_parsed": len(rows),
                    "agent_code": override_agent_code or "",
                    "agent_name": override_agent_name or "",
                    "upload_id": "",
                    "rows_inserted": 0,
                    "moved_to": "",
                    "status": "DRY_RUN",
                    "error": "",
                })
                logger.log_json(summary)
                continue

            summary = integ.process(
                doc_type_key=doc,
                agent_code=str(override_agent_code or ""),
                agent_name=override_agent_name or None,
                df_rows=rows,
                file_path=p,
                month_year_hint=None,
            )
            summary.setdefault("status", "success")
            results.append(summary)
            logger.log_json(summary)
            logger.log_csv({
                "type": summary.get("doc_type", doc.upper()),
                "file": p.name,
                "rows_parsed": len(rows),
                "agent_code": summary.get("agent_code", "") or (override_agent_code or ""),
                "agent_name": summary.get("agent_name", "") or (override_agent_name or ""),
                "upload_id": summary.get("upload_id", ""),
                "rows_inserted": summary.get("rows_inserted", 0),
                "moved_to": summary.get("moved_to", ""),
                "status": summary.get("status", "success"),
                "error": summary.get("error", ""),
            })

            # STATEMENT post-processing
            if (not dry_run and doc == "statement"
                and summary.get("upload_id") is not None
                and summary.get("status") == "success"):
                upid = _as_int(summary.get("upload_id"))
                agent = str(summary.get("agent_code") or override_agent_code or "")
                mmyy = str(summary.get("month_year") or "")
                if upid is not None and agent and mmyy:
                    rows_exp = compute_expected_for_upload_dynamic(upload_id=upid)
                    inserted = insert_expected_rows(rows_exp)
                    logger.log_csv({
                        "type": "EXPECTED_COMMISSIONS",
                        "file": p.name,
                        "rows_parsed": len(rows_exp),
                        "agent_code": summary.get("agent_code", "") or (override_agent_code or ""),
                        "agent_name": summary.get("agent_name", "") or (override_agent_name or ""),
                        "upload_id": summary.get("upload_id", ""),
                        "rows_inserted": inserted,
                        "moved_to": summary.get("moved_to", ""),
                        "status": "success",
                        "error": "",
                    })
                    _auto_deactivate_prior_active(agent, mmyy, "STATEMENT", upid)
                    _refresh_active_policies_for(agent, mmyy)

        except Exception as e:
            err = {
                "status": "api_error",
                "error": str(e),
                "doc_type": doc.upper(),
                "agent_code": override_agent_code or "",
                "agent_name": override_agent_name or "",
                "month_year": None,
                "upload_id": None,
                "rows_inserted": 0,
                "moved_to": None,
            }
            results.append(err)
            logger.log_csv({
                "type": doc.upper(),
                "file": p.name,
                "rows_parsed": "",
                "agent_code": override_agent_code or "",
                "agent_name": override_agent_name or "",
                "upload_id": "",
                "rows_inserted": "",
                "moved_to": "",
                "status": "api_error",
                "error": str(e),
            })
            logger.log_json(err)

    return {"status": "OK", "count": len(results), "results": results}
